{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb2d717f-52f2-407a-8e10-c530f5f04c1b",
   "metadata": {},
   "source": [
    "# What is Boosting?\n",
    "<img src=\"https://miro.medium.com/max/1400/1*jbncjeM4CfpobEnDO0ZTjw.png\" width=500>\n",
    "Definition: Whenever several weak learners are combined to form a strong learner, we call it boosting(boosting hypothesis), here the weak learners are sequentially produced during the training phase. The performance of the model is improved by assigning a higher weightage to the previous, incorrectly classified samples. the most popular ones are AdaBoost(Adaptive Boosting) and Gradient Boosting.\n",
    "\n",
    "\n",
    "Let’s understand this definition in detail by solving a problem of spam email identification:\n",
    "\n",
    "How would you classify an email as SPAM or not? Like everyone else, our initial approach would be to identify ‘spam’ and ‘not spam’ emails using following criteria. If:\n",
    "\n",
    "* Email has only one image file (promotional image), It’s a SPAM\n",
    "* Email has only link(s), It’s a SPAM\n",
    "* Email body consist of sentence like “You won a prize money of $ xxxxxx”, It’s a SPAM\n",
    "* Email from our official domain “Analyticsvidhya.com” , Not a SPAM\n",
    "* Email from known source, Not a SPAM\n",
    "\n",
    "Above, we’ve defined multiple rules to classify an email into ‘spam’ or ‘not spam’. But, do you think these rules individually are strong enough to successfully classify an email? No.\n",
    "\n",
    "Individually, these rules are not powerful enough to classify an email into ‘spam’ or ‘not spam’. Therefore, these rules are called as **weak learner**.\n",
    "\n",
    "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\n",
    "•   Using average/ weighted average\n",
    "•   Considering prediction has higher vote\n",
    "\n",
    "For example:  Above, we have defined 5 weak learners. Out of these 5, 3 are voted as ‘SPAM’ and 2 are voted as ‘Not a SPAM’. In this case, by default, we’ll consider an email as SPAM because we have higher(3) vote for ‘SPAM’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984b949-30c0-487e-a749-98a8dddd82a6",
   "metadata": {},
   "source": [
    "# How Boosting Algorithms works?\n",
    "Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, ‘How boosting identify weak rules?\n",
    "\n",
    "To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "\n",
    "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\n",
    "\n",
    "For choosing the right distribution, here are the following steps:\n",
    "1.  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "2. If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "3. Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2d49d-a30f-4aaf-9ad1-9a665ea9b5de",
   "metadata": {},
   "source": [
    "# Types of Boosting Algorithms\n",
    "Underlying engine used for boosting algorithms can be anything.  It can be decision stamp, margin-maximizing classification algorithm etc. There are many boosting algorithms which use but the most popular ones are AdaBoost(Adaptive Boosting) and Gradient Boosting.\n",
    "\n",
    "## 01. AdaBoost\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/11/bigd.png\">\n",
    "This diagram aptly explains Ada-boost. Let’s understand it closely:\n",
    "\n",
    "**Box 1:** You can see that we have assigned equal weights to each data point and applied a decision stump to classify them as + (plus) or – (minus). The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectly predicted three + (plus) as – (minus). In such case, we’ll assign higher weights to these three + (plus) and apply another decision stump.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd1-e1526989432375.png\">\n",
    "\n",
    "**Box 2:** Here, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points. In this case, the second decision stump (D2) will try to predict them correctly. Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has caused mis-classification errors. This time with three -(minus). Again, we will assign higher weight to three – (minus) and apply another decision stump.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd2-e1526989487878.png\">\n",
    "\n",
    "**Box 3:** Here, three – (minus) are given higher weights. A decision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and – (minus) based on higher weight of mis-classified observation.\n",
    "\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/11/dd3.png\">\n",
    "\n",
    "**Box 4:** Here, we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithm has classified these observation quite well as compared to any of individual weak learner.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd4-e1526551014644.png\">\n",
    "\n",
    "**AdaBoost (Adaptive Boosting) :** It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.\n",
    "\n",
    "You can tune the parameters to optimize the performance of algorithms, I’ve mentioned below the key parameters for tuning:\n",
    "\n",
    "* **n_estimators:** It controls the number of weak learners.\n",
    "* **learning_rate:** Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "* **base_estimators:** It helps to specify different ML algorithm.\n",
    "You can also tune the parameters of base learners to optimize its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4b6e3-4777-4514-a79a-38d2a4b8a3fd",
   "metadata": {},
   "source": [
    "# Let’s get a quick look at the implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bb834-f3e2-477a-bbf8-986495477745",
   "metadata": {},
   "source": [
    "**Importing necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "afb97cc8-2ecc-4754-96f4-5c1e4cc51fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e201d-798f-47d8-bc91-b313e151cd37",
   "metadata": {},
   "source": [
    "**Creating dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1d1f767-fd31-4192-800f-c12bd3212cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a075d34-b47e-41c1-a769-f23116dff816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree= DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    base_estimator= DecisionTree, n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "adaboost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696e752-0850-416d-951c-9cf454cdf842",
   "metadata": {},
   "source": [
    "The above code trains an AdaBoost classifier based on 200 Decision Stumps(one withmax_depth set to 1, consists of a single decision node and two leaf nodes). Here learning rate is the weight applied to each classifier during the boosting iterations. The higher the learning rate, the higher is the contribution of each classifier. Keep in mind that there is a trade between n_estimators and learning rate.\n",
    "\n",
    "Now you must be wondering what is SAMME here, so it is a multi-class version of AdaBoost which stands for **Stagewise Addictive Modelling using a Multiclass Exponential** Loss function. You might have also noticed an (.R) there, it stands for real, if the predictors with which you are working have the predict_proba() method meaning they can estimate class probabilities, you can use a. R because class probabilities generally perform better. This is the default base estimator for AdaBoost Classifier.\n",
    "\n",
    "**Prediction and Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "600e5a35-de12-44ad-9df5-a17bab6712e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109065c-b140-4350-86f1-ea541f4fc9b8",
   "metadata": {},
   "source": [
    "We got around 90% of accuracy, which is not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2b50a-2dda-4aba-98f5-54bf77712994",
   "metadata": {},
   "source": [
    "# **02. Gradient Boosting**\n",
    "Best Sources = [1. Analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/04/how-the-gradient-boosting-algorithm-works/), \n",
    "[2. Analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/09/gradient-boosting-algorithm-a-complete-guide-for-beginners/)\n",
    "\n",
    "\n",
    "\n",
    "Gradient Boosting is also based on sequential ensemble learning. Here the base learners are generated sequentially in such a way that the present base learner is always more effective than the previous one, i.e. the overall model improves sequentially with each iteration.\n",
    "\n",
    "The difference in this type of boosting is that the weights for misclassified outcomes are not incremented, instead, Gradient Boosting method tries to optimize the loss function of the previous learner by adding a new model that adds weak learners in order to reduce the loss function.\n",
    "\n",
    "The main idea here is to overcome the errors in the previous learner’s predictions. This type of boosting has three main components:\n",
    "\n",
    "* Loss function that needs to be enhance.\n",
    "\n",
    "* Weak learner for computing predictions and forming strong learners.\n",
    "\n",
    "* An Additive Model that will regularize the loss function.\n",
    "\n",
    "Like AdaBoost, Gradient Boosting can also be used for both classification and regression problems.\n",
    "When it is used as a regressor, the cost function is **Mean Square Error (MSE)** and when it is used as a classifier then the cost function is **Log loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da616f8-d65c-4ee5-a472-214f59db9d7d",
   "metadata": {},
   "source": [
    "## Manual Example for understanding the algorithm:\n",
    "Let us now understand the working of the Gradient Boosting Algorithm with the help of one example. In the following example, Age is the Target variable whereas LikesExercising, GotoGym, DrivesCar are independent variables. As in this example, the target variable is continuous, GradientBoostingRegressor is used here.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/27961GradientBoosting_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b88b0-3e04-4938-86b0-ed6bf149f78a",
   "metadata": {},
   "source": [
    "### **1st-Estimator**\n",
    "For estimator-1, the root level (level 0) will consist of all the records. The predicted age at this level is equal to the mean of the entire Age column i.e. 41.33(addition of all values in Age column divided by a number of records i.e. 9). Let us find out what is the MSE for this level. MSE is calculated as the mean of the square of errors. Here error is equal to actual age-predicted age. The predicted age for the particular node is always equal to the mean of age records of that node. So, the MSE of the root node of the 1st estimator is calculated as given below.\n",
    "\n",
    "**MSE=(∑(Agei –mu)2 )/9=577.11**\n",
    "\n",
    "The cost function hers is MSE and the objective of the algorithm here is to minimize the MSE. \n",
    "Now, one of the independent variables will be used by the Gradient boosting to create the Decision Stump.\n",
    "\n",
    "Let us suppose that the **LikesExercising** is used here for prediction. So, the records with False LikesExercising will go in one child node, and records with True LikesExercising will go in another child node as shown below.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/63935GradientBoosting_1.png\">\n",
    "\n",
    "Let us find the means and MSEs of both the nodes of level 1. For the left node, the mean is equal to 20.25 and MSE is equal to 83.19. Whereas, for the right node, the mean is equal to 58.2 and MSE is equal to 332.16. Total MSE for level 1 will be equal to the addition of all nodes of level 1 i.e. 83.19+332.16=415.35. We can see here, the cost function i.e. MSE of level 1 is better than level 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d80f93-f2bd-4ac7-98b6-8768eb28db36",
   "metadata": {},
   "source": [
    "### **2nd-Estimator:**\n",
    "Let us now find out the estimator-2. Unlike AdaBoost, in the Gradient boosting algorithm, **residues** (agei – mu)of the **first estimator** are taken as **root nodes** as shown below. Let us suppose for this estimator another independent(**GotoGym**) variable is used for prediction. So, the records with False GotoGym\n",
    "will go in one child node, and records with True GotoGym will go in another child node as shown below.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/17092GradientBoosting_2.png\">\n",
    "\n",
    "The prediction of age here is slightly tricky. First, the age will be predicted from estimator 1 as per the value of LikeExercising, and then the mean from the estimator is found out with the help of the value of GotoGym and then that means is added to age-predicted from the first estimator and that is the final prediction of Gradient boosting with two estimators.\n",
    "\n",
    "Let us consider if we want to predict the age for the following records:\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/56336GradientBoosting_5.png\">\n",
    "\n",
    "Here, LikesExercising is equal to False. So, the predicted age from the first estimator will be 20.25 (i.e. mean of the left node of the first estimator). Now we need to check what is the value of GotoGym for the second predictor and its value is True. So, the mean of True GotoGym in the 2nd estimator is -3.56. This will be added to the prediction of the first estimator i.e. 20.25. So final prediction of this model will be 20.25+(-3.56) = 16.69\n",
    "\n",
    "Let us predict of ages of all records we have in the example.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/32915GradientBoosting_3.png\">\n",
    "\n",
    "Let us now find out the Final MSE for above all 9 records.\n",
    "\n",
    "**MSE** = \n",
    "((-2.69)2 +(-1.69)2 + (-0.69)2 +(-28.64)2 +(19.31)2 +(-15.33)2 + (14.36)2 +(6.67)2 +(7.67)2 )/9 = **194.2478**\n",
    "\n",
    "So, we can see that the final MSE is much better than the MSE of the root node of the 1st Estimator. This is only for 2 estimators. There can be n number of estimators in gradient boosting algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32199a-55e1-4009-906e-f080e10142dc",
   "metadata": {},
   "source": [
    "# Implementation of the above example\n",
    "Let us now write a python code for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b317c070-72bb-4b41-b7df-2dba4b593cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07a2eb51-c1f1-408e-acee-250dd2733439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create the Data-Frame for above \n",
    "X=pd.DataFrame({'LikesExercising':[False,False,False,True,False,True,True,True,True],\n",
    "                'GotoGym':[True,True,True,True,True,False,True,False,False],\n",
    "                 'DrivesCar':[True,False,False,True,True,False,True,False,True]})\n",
    "Y=pd.Series(name='Age',data=[14,15,16,26,36,50,69,72,74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bf8b21d-68c5-4911-ae54-b7d49a552017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LikesExercising</th>\n",
       "      <th>GotoGym</th>\n",
       "      <th>DrivesCar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LikesExercising  GotoGym  DrivesCar\n",
       "0            False     True       True\n",
       "1            False     True      False\n",
       "2            False     True      False\n",
       "3             True     True       True\n",
       "4            False     True       True\n",
       "5             True    False      False\n",
       "6             True     True       True\n",
       "7             True    False      False\n",
       "8             True    False       True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da206f34-f142-4328-bdd5-aa2f3eb35b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us encode true and false to number value 0 and 1\n",
    "LE=LabelEncoder()\n",
    "X['LikesExercising']=LE.fit_transform(X['LikesExercising'])\n",
    "X['GotoGym']=LE.fit_transform(X['GotoGym'])\n",
    "X['DrivesCar']=LE.fit_transform(X['DrivesCar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e35c6-5d9b-44b6-b009-a55314d24960",
   "metadata": {},
   "source": [
    "**We will now see the effect of different numbers of estimators on MSE**\n",
    "**1.GradientBoostingRegressor with 2 estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1be412bb-c383-44d6-ad8a-33edfc116499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.23 , 36.425, 36.425, 42.505, 38.23 , 45.07 , 42.505, 45.07 ,\n",
       "       47.54 ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to train the model and to predict the age for the same inputs. \n",
    "GB=GradientBoostingRegressor(n_estimators=2)\n",
    "GB.fit(X,Y)\n",
    "\n",
    "Y_predict=GB.predict(X) #ages predicted by model with 2 estimators \n",
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c759d5d1-9f6a-400b-b5bf-0c405f65b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for two estimators = 432.48205555555546\n"
     ]
    }
   ],
   "source": [
    "# Following code is used to find out MSE of prediction\n",
    "# with Gradient boosting algorithm having estimator 2.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(f\"MSE for two estimators = {mean_squared_error(Y, Y_predict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa747cb4-7359-4db4-a70b-9279e4eb153f",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor with 3 estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acd766c6-5dfe-46ed-a7f7-f28f6d438aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36.907 , 34.3325, 34.3325, 43.0045, 36.907 , 46.663 , 43.0045,\n",
       "       46.663 , 50.186 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Let us now use GradientBoostingRegressor with 3 estimators to train the model and to predict the age for the same inputs. \n",
    "GB3=GradientBoostingRegressor(n_estimators=3)\n",
    "GB3.fit(X,Y)\n",
    "Y_predict3 =GB3.predict(X) #ages predicted by model with 3 estimators\n",
    "Y_predict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad301102-213f-4f9e-b5ce-9cdb6172fd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Three estimators = 380.0560205555555\n"
     ]
    }
   ],
   "source": [
    "# Following code is used to find out MSE of prediction\n",
    "# with Gradient boosting algorithm having estimator 3.\n",
    "\n",
    "print(f\"MSE for Three estimators = {mean_squared_error(Y, Y_predict3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa15fd2-878f-475f-9a52-3ad864f95bcc",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor with 50 estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4b72aa4-afa5-4718-b725-fa04739cfba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.08417833, 15.63313919, 15.63313919, 47.46821839, 25.08417833,\n",
       "       60.89864242, 47.46821839, 60.89864242, 73.83164334])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Let us now use GradientBoostingRegressor with 50 estimators\n",
    "# to train the model and to predict the age for the same inputs.\n",
    "GB50=GradientBoostingRegressor(n_estimators=50)\n",
    "GB50.fit(X,Y)\n",
    "\n",
    "Y_predict50 =GB50.predict(X) #ages predicted by model with 50 estimators\n",
    "Y_predict50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14e4dbb1-ff06-4e32-8f14-0e3563a56e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for 50 estimators = 156.5667260994211\n"
     ]
    }
   ],
   "source": [
    "#Following code is used to find out MSE of prediction with Gradient boosting\n",
    "# algorithm having estimator 50.\n",
    "print(f\"MSE for 50 estimators = {mean_squared_error(Y, Y_predict50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d7a17-c78c-4308-bc77-5d231d5594b6",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "As we can see here, MSE reduces as we increase the estimator value. The situation comes where MSE becomes saturated which means even if we increase the estimator value there will be no significant decrease in MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7de365-21c9-45f3-92c9-4832b920b5b1",
   "metadata": {},
   "source": [
    "## Finding the best estimator with GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec446747-1808-4018-94c6-e0abf7fff81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator returned by GridSearch CV is: GradientBoostingRegressor(n_estimators=19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model=GradientBoostingRegressor()\n",
    "\n",
    "params={'n_estimators':range(1,200)}\n",
    "\n",
    "grid=GridSearchCV(estimator=model,cv=2,param_grid=params,scoring='neg_mean_squared_error')\n",
    "grid.fit(X,Y)\n",
    "\n",
    "print(\"The best estimator returned by GridSearch CV is:\",grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fcb634e-6554-4910-95ed-bde15ae9d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.20639114, 18.98970027, 18.98970027, 46.66697477, 27.20639114,\n",
       "       58.34332496, 46.66697477, 58.34332496, 69.58721772])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The best estimator returned by GridSearch CV is:  GradientBoostingRegressor(n_estimators=19)\n",
    "GBgrid=grid.best_estimator_\n",
    "GBgrid.fit(X,Y)\n",
    "Y_predict_grid=GBgrid.predict(X)\n",
    "Y_predict_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aef12dbd-ec12-4a20-9a62-bc117c1cf3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for best estimators = 164.22985486053906\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE for best estimators = {mean_squared_error(Y, Y_predict_grid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74b4a6-8ddc-442d-bc34-a22466a4e901",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "You may think that MSE for n_estimator=50 is better than MSE for n_estimator=19. Still GridSearchCV returns 19 not 50. Actually, we can observe here is until 19 with each increment in estimator value the reduction in MSE was significant, but after 19 there is no significant decrease in MSE with increment in estimators. So, n_estimator=19 was returned by GridSearchSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a15c4-1805-480a-9fe1-f5c782c2ea52",
   "metadata": {},
   "source": [
    "### Applications:\n",
    "i) Gradient Boosting Algorithm is generally used when we want to decrease the Bias error.\n",
    "\n",
    "ii) Gradient Boosting Algorithm can be used in regression as well as classification problems. In regression problems, the cost function is MSE whereas, in classification problems, the cost function is Log-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019dab4-37af-4f4d-96c6-5c3e16afc22f",
   "metadata": {},
   "source": [
    "# XGBoost \n",
    "XGBoost is an advanced version of Gradient boosting method, it literally means eXtreme Gradient Boosting. XGBoost developed by Tianqi Chen, falls under the category of Distributed Machine Learning Community (DMLC).\n",
    "\n",
    "The main aim of this algorithm is to increase the speed and efficiency of computation. The Gradient Descent Boosting algorithm computes the output at a slower rate since they sequentially analyze the data set, therefore XGBoost is used to boost or extremely boost the performance of the model.\n",
    "\n",
    "XGBoost is designed to focus on computational speed and model efficiency. The main features provided by XGBoost are:\n",
    "\n",
    "* Parallelly creates decision trees.\n",
    "\n",
    "* Implementing distributed computing methods for evaluating large and complex models.\n",
    "\n",
    "* Using Out-of-Core Computing to analyze huge datasets.\n",
    "\n",
    "* Implementing cache optimization to make the best use of resources.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
