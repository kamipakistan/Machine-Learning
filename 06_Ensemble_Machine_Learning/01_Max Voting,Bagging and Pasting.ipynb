{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962a8159-ffcf-4663-994e-99b2bdb18143",
   "metadata": {},
   "source": [
    "best Sources = [1. Analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/12/a-detailed-guide-to-ensemble-learning/#h2_1), \n",
    "[2. Analyticsvidhya](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/#h2_1)\n",
    "# Introduction to Ensemble Learning\n",
    "\n",
    "Let’s start with an example, you have a question, and you ask it around, then aggregate their answers. In most cases, you would find that this answer is way better than one expert’s answer. So this is called the **wisdom of the crowd**. Similarly, if you aggregate the predictions of models such as classifiers or regressors, you would notice that the group has better performance than the best individual model. So this group of predictors or models are called **ensemble** and hence this technique is knowns as **Ensemble learning**.\n",
    "\n",
    "For example, you can train a group of decision trees classifiers, on different random subsets of the training data. After that to make predictions, obtain predictions from all individual trees and predict the most frequent class (i.e predicted the most). This ensemble of decision trees is called **Random Forest** and is one of the most powerful algorithms in the machine learning world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec4813-aec4-4ab5-80bf-bf9badaca12f",
   "metadata": {},
   "source": [
    "# 01. Voting Classifiers in Ensemble Learning (Max Voting, Soft Voting)\n",
    "\n",
    "* Suppose you have a couple of classifiers, with each one having an accuracy of about 80-85%. Now, you can have a Support Vector Classifier, a Random Forest Classifier, a Logistics Regression Classifier, a K-Nearest Neighbors classifier, and perhaps a couple more. A simple way of creating an even better classifier is to combine the predictions of all classifiers and output the most frequent class. This type of classification(Majority-vote Classification) is known as **Hard Voting Classifier**.\n",
    "\n",
    "* **01. Max Voting / Majority-vote Classification / Hard Voting Classifier**: The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction. **For example**, when you asked 5 of your colleagues to rate your movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n",
    "\n",
    "* **02.Soft voting** If you are working with classifiers that have the **predict_proba()** method means they can estimate the probability of a class. Then, we output the class with the highest probability which is averaged over all the classifiers. This technique is knowns as soft voting. This technique usually performs better than hard voting as it focuses more on the highly confident votes. To use this, all you need to do is replace **“hard” with “soft** and keep in mind that this will only work for the classifiers that can estimate the class probability.\n",
    "\n",
    "\n",
    "* The interesting part here is, this type of voting classifier often outperforms even the best classifier in the ensemble. In fact, even if each of the classifiers is a weak learner means it is slightly better than a model which is guessing randomly. The ensembles model can still be a strong learner, assuming there are enough weak learners and are independent of one another.  **An ensemble performs well only in cases when all the classifiers are perfectly independent of each other**, i.e. making uncorrelated errors, which is quite challenging to achieve because they are all trained on the same data. Hence, they are likely to make the same types of errors. So in turn, there will be a majority of the votes for the wrong class, reducing the overall accuracy of the ensemble. One way to achieve a diverse set of classifiers is to use very different algorithms. With different algorithms, they may make different kinds of errors, which will in turn increase the accuracy of the ensemble.\n",
    "\n",
    "* The following code initializes and trains a classifier comprising three different classifiers which will be SVM, Random Forest, and Logistics Regression. We will be using Moon’s Dataset for our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b244a5-a731-4cba-842a-546a5e7c52b9",
   "metadata": {},
   "source": [
    "## Let’s start with our implementation\n",
    "**Importing necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5649e55-f596-4a1a-86b8-0e1855256be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183758f-3f85-46f8-ae59-08458da3a23a",
   "metadata": {},
   "source": [
    "**Creating dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d2f9dd2-82e2-461f-879a-04d43482720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e608-5393-421b-bb86-0acb1a6cdb90",
   "metadata": {},
   "source": [
    "**Initializing the models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffca6c6d-8863-4063-9f7d-8613ce00d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LogisticRegression()\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dc4d8bd-40b1-4deb-b0f1-ee8707894ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(\n",
    "    estimators=[('logistics_regression', lreg),\n",
    "                ('random_forest', rf),\n",
    "                ('support_vector_machine', svm)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2eaea-80aa-4fdf-bc0f-59af5781bf12",
   "metadata": {},
   "source": [
    "Here we are setting voting = ‘hard’ means it will be following majority rule voting and n_estimators as 100. The rest of all the parameters are by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bb9be-16b4-4ce0-9927-2f463bb531f5",
   "metadata": {},
   "source": [
    "**Fitting training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "555f08d7-1348-4730-ba1c-ed102a758076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('logistics_regression', LogisticRegression()),\n",
       "                             ('random_forest', RandomForestClassifier()),\n",
       "                             ('support_vector_machine', SVC())])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df361204-fbdc-4745-8281-0a45a8055c56",
   "metadata": {},
   "source": [
    "**Now let’s see how this works on the test set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fe1a8bc-662f-4082-9fd9-d974375b26e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.832\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.888\n",
      "VotingClassifier 0.88\n"
     ]
    }
   ],
   "source": [
    "for Classifier in (lreg, rf, svm, voting):\n",
    "    Classifier.fit(X_train, y_train)\n",
    "    y_pred = Classifier.predict(X_test)\n",
    "    print(Classifier.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f308ea-589a-4286-a4a7-fd265f3e5de3",
   "metadata": {},
   "source": [
    "Boom! There we have it, as we can see the voting classifier outperforming other individual models slightly. If you are working with classifiers that have the **predict_proba()** method means they can estimate the probability of a class. Then, we output the class with the highest probability which is averaged over all the classifiers. This technique is knowns as **soft voting**. This technique usually performs better than hard voting as it focuses more on the highly confident votes. To use this, all you need to do is replace **“hard” with “soft”** and keep in mind that this will only work for the classifiers that can estimate the class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673db98d-b1ec-4319-9a7b-41d9f49452f8",
   "metadata": {},
   "source": [
    "# 02. Bagging and Pasting in Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09960cf-5a62-4808-94e7-fd1a6fb2e4df",
   "metadata": {},
   "source": [
    "### 2.1. Bagging\n",
    "\n",
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here’s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is **bootstrapping**.\n",
    "\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement\n",
    "\n",
    "**Bagging** means bootstrap+aggregating and it is a ensemble method in which we first bootstrap our data and for each bootstrap sample we train one model. After that, we aggregate them with equal weights. \n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/1280px-Ensemble_Bagging.svg.png\" width = 500>\n",
    "\n",
    "* Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "* A base model (weak model) is created on each of these subsets.\n",
    "* The models run in parallel and are independent of each other.\n",
    "* The final predictions are determined by combining the predictions from all the models\n",
    "\n",
    "### 2.2. Pasting:\n",
    "\n",
    "Whenever we select a subset of the data without replacement it is known as Pasting.\n",
    "\n",
    "Both bagging and pasting trains predictors on a different random subset of the training data. At the end, when all the predictors are trained, the ensemble can predict new data by simply combining the prediction of all the predictors the same as the hard voting classifier(As mode in statistics). Most of the time ensemble has the same bias but less variance than an individual predictor trained on the original training data.\n",
    "\n",
    "For more information on Bias and Variance check out this blog [here](https://www.analyticsvidhya.com/blog/2021/06/how-to-get-the-most-out-of-bias-variance-tradeoff/)\n",
    "\n",
    "The biggest advantage of bagging and pasting is that can they can be trained to different CPUs or on different servers as well and this is one of many other reasons why both are very popular methods.\n",
    "\n",
    "The biggest advantage of bagging and pasting is that can they can be trained to different CPUs or on different servers as well and this is one of many other reasons why both are very popular methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27e4f5-c295-41da-a9c5-0d59a0816034",
   "metadata": {},
   "source": [
    "## Let’s start with our implementation:\n",
    "We will be using the same dataset that we created earlier.\n",
    "\n",
    "**01. Importing necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72cdce6f-e283-4f88-b652-c60646a0173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04378f5f-8e9d-4710-ac01-0d672d945875",
   "metadata": {},
   "source": [
    "**02. Initializing a bagging classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b6d29eb-3ba1-4745-aa83-ce3cbae77562",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=250,\n",
    "    max_samples=100, bootstrap=True, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed8265-8ef1-457f-ae9c-c898cc913a2d",
   "metadata": {},
   "source": [
    "Let’s break down the code a little bit here, the above code(**n_estimators=250**) will train an ensemble of 250 decision tree classifiers, and each of them is trained on 100 random subsets of the training data(**max_samples=100**)with replacement as we are working on a bagging classifier but if you wish to work on pasting instead, go ahead and set the **bootstrap = False**. Lastly, the **n_jobs** parameters specify how many CPU cores you want to use for training and prediction(-1 means it can use all available).\n",
    "\n",
    "**For regression, you can use BaggingRegressors.**\n",
    "\n",
    "Check out the official documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) for more information.\n",
    "\n",
    "##### **03. Training the classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38970809-9543-4b00-8212-4b9ef2304684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=250, random_state=101)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d885ad2-9501-4ebd-86e8-12ec2f280bd7",
   "metadata": {},
   "source": [
    "##### **04. Testing the classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5901e231-534d-42db-a848-5fd660b9189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888\n"
     ]
    }
   ],
   "source": [
    "y_pred = bagging_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609279f-0d7d-4122-9e75-75851607eac9",
   "metadata": {},
   "source": [
    "If we compare a regular decision tree classifier with the bagging classifier we just created, both of them are trained on moons dataset. By observing, the below figure we can see that\n",
    "\n",
    "<img src = \"https://editor.analyticsvidhya.com/uploads/67391fig.PNG\" width=500>\n",
    "The ensemble is much more generalized than an individual decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd53b0-b15a-4005-9962-b88f502b5cde",
   "metadata": {},
   "source": [
    "# Out-Of-Bag Evaluation\n",
    "\n",
    "Whenever you use bagging, there is some amount of data that might be sampled(selecting subset) more than once, or perhaps even more and there might be some data that is not sampled at all. Bagging Classifier samples n training instances with replacement by default, which means only around 63% of data is sampled on an average for each predictor. The rest of the 37% of the data are not sampled at all, now these data/instances are knowns as out-of-bag(OOB) instances. Remember this 37% are not the same for all predictors.\n",
    "\n",
    "Since our model never saw this data before, it can be useful for evaluating removing the need for separate validation data.\n",
    "\n",
    "Bagging classifier has a parameter oob_score which upon setting true will automatically use oobs for evaluation after training is done.\n",
    "\n",
    "### Let’s get a quick look at the implementation:\n",
    "The libraries and datasets used here are the same.\n",
    "\n",
    "**Initializing the classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b0b7f52d-2ee6-473e-adbc-701bb7aeb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=250,\n",
    "    bootstrap=True, oob_score=True, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c3bec-e6eb-47a5-ad38-9fec34f9ac1d",
   "metadata": {},
   "source": [
    "**Training the Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f986ad56-10c0-4c4c-b6cf-a19892c33812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=250,\n",
       "                  oob_score=True, random_state=101)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff1782-16ee-47b2-9670-847f6df2e9b1",
   "metadata": {},
   "source": [
    "**Result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c52cf051-de10-4f6d-ad44-16d005070470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692300fe-e904-4ed0-9b4d-6b4c0c5d5009",
   "metadata": {},
   "source": [
    "we get 91% which is close enough to what we achieved earlier(0.888). So by testing the model on oobs we can get a rough estimate of the accuracy we will achieve on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb42a3-9d4c-4e12-a82f-e2f719e48a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
